{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce56516",
   "metadata": {},
   "source": [
    "# AR PROJECT \n",
    "\n",
    "\n",
    "### Michael Albarello and Matteo Nestola"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53382eaf",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Augmented Reality, whose acronym is AR, is an immersive technology capable of enhancing the sensory experience of users through technological tools. In other words, this technology allows to merge real and virtual elements to intensify the user's perception of the reality. \n",
    "\n",
    "In our specific case, we were demanded to superimpose a layer containing the course’s logo and an author’s name on top of the cover of a well-known Computer Vision book in order to appear real to the observer. In particular, the professor provided us the files:\n",
    "\n",
    "- Input video sequence to be augmented;\n",
    "- First frame of the input sequence;\n",
    "- Binary mask that identifies the pixels belonging to the book in the reference frame;\n",
    "- Image containing the augmented reality layer;\n",
    "- Binary mask of the augmented reality layer.\n",
    "\n",
    "In the following sections we will explain step by step how we obtained the final outcome starting from the previous files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a792ebbd",
   "metadata": {},
   "source": [
    "## Libraries \n",
    "First of all, we had to import the appropriate libraries to make use of the most important computer vision tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b516aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08d9d52",
   "metadata": {},
   "source": [
    "## Input data \n",
    "All of the given media files listed in the introduction, have been stored in the following python variables:\n",
    "\n",
    "* `ref_frame`, which is the first frame of the video and it is used as anchor to find correspondences with all the other frames;\n",
    "\n",
    "* `video`, which is the media file that will be augmented by extracting each frame and adding on top of each of them the transformed augmented layer;  \n",
    "\n",
    "* `aug_layer`, which is the layer that will be added onto the original video; \n",
    "\n",
    "* `aug_layer_mask`, which is the binary mask where white represents the area of interest and black the background that will be converted to transparent;\n",
    "\n",
    "* `ref_mask`, which is the binary mask of the reference frame;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e5a2310",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error opening video stream or file\n"
     ]
    }
   ],
   "source": [
    "#video to augment\n",
    "video = cv2.VideoCapture('./Data/Multiple_View.avi')\n",
    "\n",
    "if (video.isOpened() == False):\n",
    "    print(\"Error opening video stream or file\")\n",
    "\n",
    "#reference frame and associated mask\n",
    "ref_frame = cv2.imread('./Data/ReferenceFrame.png')\n",
    "ref_mask = cv2.imread('./Data/ObjectMask.PNG') \n",
    "\n",
    "#augmented layer and associated mask\n",
    "aug_layer = cv2.imread('./Data/AugmentedLayer.PNG')\n",
    "aug_layer_mask = cv2.imread('./Data/AugmentedLayerMask.PNG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529af917",
   "metadata": {},
   "source": [
    "## Resizing and reshaping the augmented layer's mask\n",
    "Now we resized the `aug_layer_rgba` to be of the same size of the `reference_rgba`, so that the two images would fit perfectly one on top of the other. Then we reshaped the augmented layer mask in order to build a mask to select only the words and logo and not their background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0798fca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#resizing the mask\n",
    "\n",
    "h_r, w_r, channels = ref_frame.shape\n",
    "\n",
    "aug_layer_resized = aug_layer[0:0 + h_r, 0:0 + w_r]\n",
    "aug_layer_mask_resized = aug_layer_mask[0:0 + h_r, 0:0 + w_r]\n",
    "\n",
    "cv2.imshow('aug_layer_resized', aug_layer_resized)\n",
    "\n",
    "\n",
    "#reshaping the mask \n",
    "\n",
    "# diff = cv2.bitwise_and(ref_frame, aug_layer_mask_resized)         #\n",
    "layer_gray = cv2.cvtColor(aug_layer_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Creating kernel\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "kernel2 = np.ones((5, 5), np.uint8)\n",
    "\n",
    "#Binary Morphology Closing (dilation -->erosion)\n",
    "layer_gray = cv2.dilate(layer_gray, kernel)\n",
    "layer_gray = cv2.erode(layer_gray, kernel2)\n",
    "\n",
    "_, im_gray_th_otsu = cv2.threshold(layer_gray, 128, 255, cv2.THRESH_OTSU)\n",
    "aug_layer_mask = cv2.cvtColor(im_gray_th_otsu, cv2.COLOR_GRAY2BGR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88301af1",
   "metadata": {},
   "source": [
    "## Defining SIFT object\n",
    "At this point, we created a SIFT object in order to detect and compute the salient points initially of the reference frame and successively of all the rest of the frames of the video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67f553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "kp_reference = sift.detect(ref_frame)                                  #detects salient points \n",
    "\n",
    "kp_reference, des_reference = sift.compute(ref_frame, kp_reference)    #for each salient point, computes the descriptor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb4ac55",
   "metadata": {},
   "source": [
    "# Functions to iterate\n",
    "From now on, we will start to iterate the same procedure for the totality of frames of the video. To make things clearer, we decided to divide the code in blocks, each one inside of a different function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a19a5f",
   "metadata": {},
   "source": [
    "## Detecting and computing the current frame's salient points \n",
    "By using the `computeFrames()` function, we simply detect and compute the salient points of the current frame of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b2583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeFrame():\n",
    "    kp_currentFrame = sift.detect(current_frame)\n",
    "    kp_currentFrame, des_currentFrame = sift.compute(current_frame, kp_currentFrame)\n",
    "    return kp_currentFrame, des_currentFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fc0d79",
   "metadata": {},
   "source": [
    "## Finding corrispondences by using FLANN's matching algorithm\n",
    "Once salient points and their associated descriptors have been found, we will have to find matches between the two images of the same scene. A common way to compute feature matching is by using FLANN (Fast Library for Approximate Nearest Neighbors), which is a library that contains a collection of algorithms optimized for fast nearest neighbor search in large datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3eceff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flann_algorithm():\n",
    "    \n",
    "    #creating the FLANN object and setting the algorithm\n",
    "    \n",
    "    FLANN_INDEX_KDTREE = 1\n",
    "    index_params = dict(algorithm=FLANN_INDEX_KDTREE, trees=5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)  # create matcher\n",
    "    \n",
    "    #finding matches between the descriptors of the reference frame and the ones of the current frame.\n",
    "    \n",
    "    matches = flann.knnMatch(des_reference, des_currentFrame, k=2)  #k defines the number of best matches to consider\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00286fb",
   "metadata": {},
   "source": [
    "## Validating matches\n",
    "The best matches are found by calculating the distance between the descriptors and selecting the k matches with smaller distance, indeed it is common that a descriptor has multiple similar good matches. To filter the matches, Lowe (SIFT paper) proposed to use a distance ratio test to try to eliminate false matches: The distance ratio between the two nearest matches of a considered keypoint is computed and it is a good match when this value is below the 0.7 threshold. \n",
    "In the following function, we check if the distance ratio is not verified so that the current match will be discarded. Once all the ambiguous matches have been filtered, it is possible to draw the keypoints to see how they change throughout the frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e213e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_and_draw_kp():\n",
    "\n",
    "#filtering keypoints to keep only good matches, by discarding the ones that don't verify Lowe's threshold.\n",
    "    \n",
    "    good = []\n",
    "    matchesMask = [[0, 0] for i in range(len(matches))]\n",
    "\n",
    "    for i, (m, n) in enumerate(matches):   \n",
    "        if m.distance < 0.7 * n.distance:\n",
    "            matchesMask[i] = [1, 0]\n",
    "            good.append(m)\n",
    "    \n",
    "#Drawing results for current frame\n",
    "\n",
    "    draw_params = dict(matchColor=(255, 0, 255),\n",
    "                       singlePointColor=(0, 255, 0),\n",
    "                       matchesMask=matchesMask,\n",
    "                       flags=cv2.DrawMatchesFlags_DEFAULT)\n",
    "    \n",
    "    kp_matches = cv2.drawMatchesKnn(current_frame, kp_currentFrame, ref_frame, kp_reference, matches, None, **draw_params)\n",
    "\n",
    "    cv2.imshow('kp correspondences', kp_matches)\n",
    "    \n",
    "        \n",
    "    return matchesMask, good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad89b1e",
   "metadata": {},
   "source": [
    "## Computing the homography\n",
    "At this point, we have to find the *homography* that maps all salient points of the reference frame (`src_pts`) to the corresponding salient points of the frame we are considering at current iteration (`dst_pts`). In order to do so, we used the `cv2.findHomography()` function, which takes as parameters the source and destination points (in `CV_32FC2` or `Point2f` format) and the algorithm used to compute the homography, in our case the `cv2.Ransac` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494c616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_homography():\n",
    "    \n",
    "#casting and reshaping the source and destination points in order to use the findHomography() function\n",
    "\n",
    "    src_pts = np.float32([kp_reference[m.queryIdx].pt for m in good]).reshape(-1, 1, 2)   \n",
    "    dst_pts = np.float32([kp_currentFrame[m.trainIdx].pt for m in good]).reshape(-1, 1, 2)     \n",
    "\n",
    "    H, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)\n",
    "    matchesMask = mask.ravel().tolist()\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc53bb16",
   "metadata": {},
   "source": [
    "## Applying the homography to the corners of the reference frame\n",
    "With the following function we define the four corners of the reference frame and we transform them using the previously found homography obtaining the projection area where the augmented layer will be superimposed.\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6b4a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perspectivePts():    \n",
    "    \n",
    "    pts = np.float32([[0, 0], [0, h_r - 1], [w_r - 1, h_r - 1], [w_r - 1, 0]]).reshape(-1, 1, 2)\n",
    "    dst = cv2.perspectiveTransform(pts, H)\n",
    "    \n",
    "    return dst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b17de9",
   "metadata": {},
   "source": [
    "## Computing final homography\n",
    "Now we are able to compute the homography that transforms the four corners of the augmented layer into the previous four transformend corners. This final homography is capable of projecting the augmented layer in a way that is compatible with the perspective of the current frame. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8500565",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_homography():\n",
    "        \n",
    "    \n",
    "    pts_layer = np.float32([[0, 0], [0, h_l - 1], [w_l - 1, h_l - 1], [w_l - 1, 0]]).reshape(-1, 1, 2)\n",
    "    H = cv2.getPerspectiveTransform(pts_layer, dst)\n",
    "    return H"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7fe620",
   "metadata": {},
   "source": [
    "## The warping process\n",
    "At this point we can use the previously generated homography function in order to compute the correct warping of the augmented layer and its related mask. Then we created a new mask where `True` corresponds to the black pixels of the `warped_aug_layer_mask` so that we can substitute all of these selected pixels with the pixels of the same coordinates of the current frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43ddb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warping_process():\n",
    "    \n",
    "    warped_aug_layer = cv2.warpPerspective(aug_layer_resized, H, (w_r, h_r))\n",
    "    #cv2.imshow(\"warped augmented layer\", warped_aug_layer)\n",
    "\n",
    "    warped_aug_layer_mask = cv2.warpPerspective(aug_layer_mask, H, (w_l, h_l))\n",
    "    #cv2.imshow(\"warped augmented layer mask\", warped_aug_layer_mask)\n",
    "\n",
    "    #creating a boolean mask instead of a binary image\n",
    "    warped_aug_layer_mask = np.equal(warped_aug_layer_mask, 0) #True for all black pixel of the mask\n",
    "    \n",
    "\n",
    "    #substituting the black pixels of the mask with the pixels of the current frame\n",
    "    warped_aug_layer[warped_aug_layer_mask] = current_frame[warped_aug_layer_mask]\n",
    "\n",
    "\n",
    "    cv2.imshow('Frame', warped_aug_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a76424",
   "metadata": {},
   "source": [
    "# Iteration of the frames of the video\n",
    "In the following code, we execute all of the previously defined functions, obtaining the augmented reality video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "180b0e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can't read frame. Exiting loop...\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    \n",
    "#Reading current iteration's frame\n",
    "\n",
    "    success, current_frame = video.read()   #success is true, if frame correctly read\n",
    "\n",
    "    if not success:\n",
    "        print(\"Can't read frame. Exiting loop...\")\n",
    "        break\n",
    "        \n",
    "        \n",
    "#Detecting and computing the salient points of the current frame\n",
    "    \n",
    "    kp_currentFrame, des_currentFrame = computeFrame()\n",
    "    \n",
    "    \n",
    "#Finding corrispondences by using FLANN's matching algorithm\n",
    "    \n",
    "    matches = flann_algorithm()    \n",
    "    \n",
    "    \n",
    "#Filtering keypoints and showing the matches of each iteration    \n",
    "\n",
    "    matchesMask, good = filter_and_draw_kp()\n",
    "        \n",
    "        \n",
    "#Checking if the keypoints are at least four. \n",
    "  \n",
    "    MIN_MATCH_COUNT = 4\n",
    "    \n",
    "    if len(good) > MIN_MATCH_COUNT:\n",
    "        \n",
    "        \n",
    "#Finding the src_pts and dst_pts, then compute the current homography\n",
    "        \n",
    "        H = find_homography()\n",
    "        \n",
    "        \n",
    "#Applying the homography to the corners of the reference frame, \n",
    "           \n",
    "        dst = get_perspectivePts()\n",
    "        \n",
    "        \n",
    "\n",
    "# trasformazione di prospettiva tra currentFrame e reference\n",
    "        \n",
    "        h_l, w_l, _ = aug_layer_resized.shape\n",
    "    \n",
    "        \n",
    "# 14) Fornendo i corner dell'augmented layer (resized) e i corner del currentFrame, calcolo la nuova homography\n",
    "            \n",
    "        H = get_last_homography()\n",
    "        \n",
    "# 15) Warping dell'Augmented_Layer\n",
    "\n",
    "        warping_process()\n",
    "        \n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        print(\"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT))\n",
    "        matchesMask = None\n",
    "        break\n",
    "        \n",
    "# 18) Una volta completati i frame del video termino le finestre\n",
    "    \n",
    "    # When everything done, release the capture\n",
    "    \n",
    "video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e40784",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "As we have seen, the outcome is valid, making the augmented reality very realistic. This result has been achieved thanks to a better use of the involved variables and algorithms:\n",
    "\n",
    "* Otsu's algorithm in order to compute the binarization of the mask;\n",
    "* Flann's based matcher for the kd-tree algorithm to select k best matches between keypoints;\n",
    "* Lowe's ratio between the distances of the best two matches to achieve a better matching process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
